#install.packages('tidyverse')
library(tidyverse)
#install.packages('microseq')
library(microseq)
#install.packages('gtools')
library(gtools)
#install.packages('muscle')
library(muscle)

#In FASTQ files, quality scores are encoded into a compact form, 
#which uses only 1 byte per quality value. In this encoding, 
#the quality score is represented as the character with an ASCII code equal to its value + 33.
#A Phred quality score is a measure of the quality of the identification of the nucleobases generated by automated DNA sequencing.
#Phred quality scores Q are defined as a property which is logarithmically related to the base-calling error probabilities P
#For example, if Phred assigns a quality score of 30 to a base, the chances that this base is called incorrectly are 1 in 1000.

#here is code for getting the quality information into a numeric vector, matches the positions in the 
#sequence strings, these could be passed in to a stastical model along with the 
phred2numeric = function(phred_string){
	outvec = asc(unlist(strsplit(phred_string, "")))
	return(outvec - 33)
}


#take a sanger-hts pair, align them to one another and then characterize the errors
pairwise_align = function(s1, s2){
	align = DNAStringSet(muscle::muscle(DNAStringSet(c(s1, s2)), maxiters = 2))
	a1 = unlist(strsplit(tolower(as.character(align[[1]])),"")) 
	a2 = unlist(strsplit(tolower(as.character(align[[2]])),""))
	return(list(a1=a1, a2=a2))
}


#take the alignment output and trim them to remove the trailing dashes (from the HTs) 
#and the corresponding bases in the alignment pair
trim_align = function(a1, a2){
	for(i in length(a2):1){
		if(a2[i] != '-'){
			return(list(a1=a1[1:i], a2=a2[1:i]))
		}
	}
}


#this function will take in an aligned pair of sanger-hts sequences
#and count the number of insertions, deletions and mutations for each bp
characterize_errors = function(a1, a2){

	pos_in_sanger = 1
	insertions = list(a= 0, t= 0, g= 0, c= 0)
	deletions  = list(a= 0, t= 0, g= 0, c= 0)
	
	#row = the sanger base
	#column = what it mutated to in the HTS data
	mutations = data.frame(a=rep(0,4),
								t=rep(0,4),
								g=rep(0,4),
								c=rep(0,4),
								row.names = c('a', 't','g','c'))

	insertion_pos = c()
	deletion_pos = c()
	mutation_pos = c()

	for(i in 1:length(a1)){
		if(a1[i] != a2[i]){
			#inserted base in HTS 
			if(a1[i] == '-'){
				insertions[a2[i]] = insertions[[a2[i]]] + 1 
				insertion_pos = c(insertion_pos, pos_in_sanger)
			#deleted base in HTS
			}else if(a2[i] == '-'){
				deletions[a1[i]] = deletions[[a1[i]]] + 1 
				deletion_pos = c(deletion_pos, pos_in_sanger)
				pos_in_sanger = pos_in_sanger + 1	
			#mutation			
			} else {
				mutations[a1[i], a2[i]] = mutations[a1[i], a2[i]] + 1
				mutation_pos = c(mutation_pos , pos_in_sanger)				
				pos_in_sanger = pos_in_sanger + 1	
			}
		} else {
				pos_in_sanger = pos_in_sanger + 1	
		}
	}

	bp_dat = list(insertions = insertions, deletions = deletions, mutations = mutations)
	error_positions = list(insertion_pos = insertion_pos , deletion_pos = deletion_pos , mutation_pos = mutation_pos )
	return(list(bp_dat = bp_dat, error_positions = error_positions))
}


#when an insertion or deletion is found, look at the complimentary 
#alignment string and count the number of same base pairs in either direction
#from the index position to characterize the length of the homopolymer
hpol_count = function(i, align_compliment){
	#count same before
	same_before = 0
	for(x in i-1:1){
		if(align_compliment[x] == align_compliment[i]){
			same_before = same_before + 1
		} else {
			break
		}
	}
	#count same after
	same_after = 0
	for(x in i+1:length(align_compliment)){
		if(align_compliment[x] == align_compliment[i]){
			same_after = same_after + 1
		} else {
			break
		}
	}
	return(same_before + same_after)
}


#this function takes an aligned pair of sanger-hts sequences and determines how many of the
#indels are associated with homopolymers in the true sequence
homopolymer_indels = function(a1, a2){

	hpols_in = data.frame(non_hp=rep(0,4),
						two_hp=rep(0,4),
						three_hp=rep(0,4),
						quad_plus_hp=rep(0,4),
						row.names = c('a', 't','g','c'))

	hpols_del = data.frame(non_hp=rep(0,4),
						two_hp=rep(0,4),
						three_hp=rep(0,4),
						quad_plus_hp=rep(0,4),
						row.names = c('a', 't','g','c'))

	for(i in 1:length(a1)){
		#insertion
		if(a1[i] == '-'){
			#look at a2[i], this is the inserted base
			hpol_len = hpol_count(i, a2)

			if (hpol_len == 2){
				hpols_in[a2[i], 'two_hp'] = hpols_in[a2[i], 'two_hp'] + 1
			}else if(hpol_len == 3){
				hpols_in[a2[i], 'three_hp'] = hpols_in[a2[i], 'three_hp'] + 1

			}else if(hpol_len > 3){
				hpols_in[a2[i], 'quad_plus_hp'] = hpols_in[a2[i], 'quad_plus_hp'] + 1
			}else{
				hpols_in[a2[i], 'non_hp'] = hpols_in[a2[i], 'non_hp'] + 1
			}
		#deletion
		} else if (a2[i] == '-'){
			#look at a1[i], this is the deleted base
			hpol_len = hpol_count(i, a1)

			if (hpol_len == 2){
				hpols_del[a1[i], 'two_hp'] = hpols_del[a1[i], 'two_hp'] + 1
			}else if(hpol_len == 3){
				hpols_del[a1[i], 'three_hp'] = hpols_del[a1[i], 'three_hp'] + 1

			}else if(hpol_len > 3){
				hpols_del[a1[i], 'quad_plus_hp'] = hpols_del[a1[i], 'quad_plus_hp'] + 1
			}else{
				hpols_del[a1[i], 'non_hp'] = hpols_del[a1[i], 'non_hp'] + 1
			}
		}
	}
	return(list(insertions=hpols_in, deletions = hpols_del))
}


#This data is from a Thermo Ion S5 sequencer. 
#Metabarcoding data from a malaise trap ontario provincial parks dataset
data = readFastq('/home/cnuge/Documents/barcode_data/mBRAVE_raw_read_data/GMP-03299)CCDB-S5-0084)CBGMB-00030.fastq')

#on server
#data = readFastq('/home/cnugent/barcode_data/mBRAVE_raw_read_data/GMP-04500)CCDB-S5-0084)CBGMB-00030.fastq')
head(data)
names(data)
#Columns are header, sequence and quality.... can the quality column be leveraged?

phred_string = data$Quality[1]
phred_string

test = head(data)
test$qual_vec = lapply(test$Quality , function(x){phred2numeric(x)})
head(test)

#is using the phred scores as an input feature, would pass them in as another tensor to the input of the model

#if using artifical training data, would need to find the distribution of phred scores for inserted data, and the 
#distribution of phred scores of data next to deletions and then randomly assign phred scores based on these ditributions.
#^maybe best to go with real world data if using a phred layer on the input

####
# try to align the reads to one another

#a hypothetical 'true' sanger sequence
s1 = 'actttatactttctctttcggaagatgggcaggtatagttggaacctctttgaagcttacttattcgtgccgaactgggaaatcctgggacattaatcggagatgaccaaatttacaatgttattgtaactgcacatgcatttgtaataattttctttatagtaatacctattatgattggagggtttggaaattggctggtacccctgatacttggcgcccctgacatagcattcccccgaataaataatataagattctgattattacccccttccctaactctccttttaataagaagccttgtagaaaggggggccggtaccggatggacagtatacccgcccctatctgccaatattgcccatagaggggcttctgtagacttagccatttttagcctccacttagccggtatctcatcaattttgggagctgtgaattttattactaccgttattaacatacgttctacaggaatgacctttgaccgaatacccctatttgtttgatcagtagctttaactgcccttcttcttcttctgtctcttcccgtattagcaggcgcaatcactatacttttaacagaccgaaatattaatacgtcattctttgaccctgcgggaggaggagaccccattctataccaacatttattt'
#two hypothetical HTS datasets, with errors
s2 = 'actttatacttttctctttcggaagatggcaggtatagttgggaacctctttgaagctttacttattcgtgcccgaactgggaatcctgggacattaatcggagatgaccaaaattacaatgttattgtaactgcacatgcatttgtaataattttctttatagtaatacctattatgattggagggtttggaaattggctggtacccctgatacttggcgcccctgacatagcattcccccgaataaataatataagattctgattattacccccttccctaactctccttttaataagaagccttgtagaaaggggggccggtaccggatggacagtatacccgcccctatctgccaatattgcccatagaggggcttctgtagacttagccatttttag'
s3 = 'acttttatacatttctcttcggaagatggggcaggtatagttggaacctctttgaagcttacttattcgtgccgaaactgggaaatcctgggacatttaatcggagatgaccaaatttacaatgttattgtaactgcacatgcatttgtaataattttctttatagtaatacctattatgattggagggtttggaaattggctggtacccctgatacttggcgcccctgacatagcattcccccgaataaataatataagattctgattattacccccttccctaactctccttttaataagaagccttgtagaaaggggggccggtaccggatggacagtatacccgcccctatctgccaatattgcccatagaggggcttctgtagacttagccatttttagcctccacttagccggtatctcatcaattttgggagctgtgaattttattactaccgttattaacatacgttctacaggaat'

align_test = pairwise_align(s1, s2)

a1 = align_test$a1
a2 = align_test$a2

a_trimmed = trim_align(a1,a2)

error_dat = characterize_errors(a_trimmed$a1, a_trimmed$a2)

hpol_dat = homopolymer_indels(a_trimmed$a1, a_trimmed$a2)



#TODO add some unit tests for the above code to the bottom here!
#then it can be repurposed for the eventual error characterization.
#likely want to have a script file to generate the output df with the stats, and
#then another one to graph/stastically analyze the information.